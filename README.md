# üöÄ Projeto: Implementa√ß√£o do Perceptron Simples

## üéì Vis√£o Geral do Projeto

Este projeto implementa o algoritmo de treinamento do **Perceptron Simples** em Python utilizando a biblioteca NumPy. O Perceptron, o bloco de constru√ß√£o mais fundamental das redes neurais, √© um modelo de classifica√ß√£o bin√°ria e linear supervisionada, desenvolvido por Frank Rosenblatt em 1957.

O objetivo √© demonstrar o aprendizado dos pesos para um conjunto de dados linearmente separ√°vel, simulando o exerc√≠cio pr√°tico da aula de [Nome da Disciplina].

---

## üß† Fundamentos Te√≥ricos do Perceptron

O Perceptron opera atrav√©s de uma fun√ß√£o de ativa√ß√£o de passo (step function) ap√≥s calcular o *net input*.

### 1. Net Input (Entrada L√≠quida)

O net input ($net$) √© a soma ponderada dos atributos de entrada ($\mathbf{x}$) pelos seus respectivos pesos ($\mathbf{w}$):

$$\text{net} = w_0 + \sum_{j=1}^{m} w_j x_j = \mathbf{w}^T \mathbf{x}$$

### 2. Fun√ß√£o de Ativa√ß√£o (Output)

A sa√≠da ($\hat{y}$) do Perceptron √© determinada pela fun√ß√£o sinal (sign function):

$$\hat{y} = \begin{cases} +1 & \text{se } \text{net} \ge 0 \\ -1 & \text{se } \text{net} < 0 \end{cases}$$

### 3. Regra de Aprendizagem (Atualiza√ß√£o de Pesos)

O Perceptron √© treinado usando a regra delta (ou regra Perceptron), que s√≥ atualiza os pesos em caso de erro ($\hat{y} \neq y$).

O ajuste do peso ($\Delta w_j$) e a atualiza√ß√£o do novo peso ($w_{j, novo}$) s√£o dados por:

$$\Delta w_j = \alpha \cdot (y - \hat{y}) \cdot x_j$$
$$w_{j, novo} = w_{j, antigo} + \Delta w_j$$

Onde $\alpha$ √© a **taxa de aprendizagem**.

---

## üõ†Ô∏è Detalhes da Implementa√ß√£o

### Estrutura do Arquivo

| Arquivo | Descri√ß√£o |
| :--- | :--- |
| `perceptron_algorithm.py` | Cont√©m a fun√ß√£o principal `perceptron_train()` e o script de teste. |
| `README.md` | Este documento. |

### Dados de Treinamento

O dataset utilizado inclui o termo de **bias ($X_0=1$)** adicionado ao vetor de atributos, resultando em $X = [X_0, X_1, X_2]$.

| Exemplo | $X_1$ | $X_2$ | Classe $Y$ | Vetor de Entrada ($X$) |
| :-----: | :---: | :---: | :--------: | :-------------------: |
| 0 | 1 | 1 | 1 | `[1, 1, 1]` |
| 1 | 0 | 1 | 1 | `[1, 0, 1]` |
| 2 | 0 | 0 | -1 | `[1, 0, 0]` |
| 3 | 1 | 0 | -1 | `[1, 1, 0]` |

### Par√¢metros

* **Pesos Iniciais ($\mathbf{w}$):** `[0.0, 0.0, 0.0]`
* **Taxa de Aprendizagem ($\alpha$):** `0.5`

---

## üìä Resultados e Converg√™ncia

O treinamento demonstrou **converg√™ncia** em 4 √©pocas, classificando corretamente todos os exemplos.

### Logs Detalhados de Atualiza√ß√£o de Pesos

A tabela mostra apenas os passos onde ocorreu erro de classifica√ß√£o e os pesos foram ajustados:

| √âpoca | Exemplo (i) | $x$ (com bias) | $y_{verdadeiro}$ | $y_{predito}$ | $w_{antigo}$ | $\Delta w$ | $w_{novo}$ |
| :---: | :---------: | :------------: | :--------------: | :-----------: | :----------: | :--------: | :--------: |
| **1** | 2 | [1, 0, 0] | -1 | 1 | [0.0, 0.0, 0.0] | [-1, 0, 0] | **[-1.0, 0.0, 0.0]** |
| **2** | 0 | [1, 1, 1] | 1 | -1 | [-1.0, 0.0, 0.0] | [1, 1, 1] | [0.0, 1.0, 1.0] |
| **2** | 2 | [1, 0, 0] | -1 | 1 | [0.0, 1.0, 1.0] | [-1, 0, 0] | [-1.0, 1.0, 1.0] |
| **2** | 3 | [1, 1, 0] | -1 | 1 | [-1.0, 1.0, 1.0] | [-1, -1, 0] | **[-2.0, 0.0, 1.0]** |
| **3** | 0 | [1, 1, 1] | 1 | -1 | [-2.0, 0.0, 1.0] | [1, 1, 1] | [-1.0, 1.0, 2.0] |
| **3** | 3 | [1, 1, 0] | -1 | 1 | [-1.0, 1.0, 2.0] | [-1, -1, 0] | **[-2.0, 0.0, 2.0]** |
| **4** | **Converg√™ncia** | N/A | N/A | N/A | N/A | N/A | **[-2.0, 0.0, 2.0]** |

### Pesos Finais

$$\mathbf{w}_{final} = [-2.0, 0.0, 2.0]$$

### Equa√ß√£o do Hiperplano (Fronteira de Decis√£o)

O hiperplano de separa√ß√£o linear √© definido pela equa√ß√£o $\mathbf{w}^T \mathbf{x} = 0$:

$$-2.0 \cdot 1 + 0.0 \cdot x_1 + 2.0 \cdot x_2 = 0$$
$$2x_2 = 2$$
$$\mathbf{x_2 = 1}$$

A fronteira de decis√£o √© uma linha horizontal em $x_2=1$.

---

## üë®‚Äçüíª Autores

* [Seu Nome / Matr√≠cula]
* [Nome da Dupla / Matr√≠cula (se aplic√°vel)]
