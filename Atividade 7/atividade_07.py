# -*- coding: utf-8 -*-
"""Atividade 07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ojstKhwboVfoW_KgLIo7SM4zUgwJy24
"""

import numpy as np
from typing import List, Tuple, Dict

def perceptron_train(
    X: np.ndarray,
    y: np.ndarray,
    w_initial: np.ndarray,
    alpha: float,
    epochs: int = 100
) -> Tuple[np.ndarray, List[Dict]]:
    """
    Implementa o algoritmo de treinamento do Perceptron.
    """
    w = w_initial.copy()
    num_examples = X.shape[0]
    log_updates = []

    print(f"--- Início do Treinamento do Perceptron ---")
    print(f"Pesos Iniciais (w): {w}")
    print(f"Taxa de Aprendizagem (α): {alpha}")
    print(f"Épocas Máximas: {epochs}\n")

    for epoch in range(1, epochs + 1):
        errors = 0

        for i in range(num_examples):
            x_i = X[i]  # Vetor de atributos do exemplo i (com bias)
            y_i = y[i]  # Rótulo verdadeiro do exemplo i

            # 1. Cálculo da saída (net input): net = w.T @ x
            net_input = np.dot(w, x_i)

            # 2. Função de Ativação (sinal): y_hat = sign(net)
            y_hat = np.where(net_input >= 0, 1, -1)

            # 3. Verificação de Erro e Atualização de Peso
            if y_hat != y_i:
                errors += 1

                # Cálculo da atualização do peso (Δw)
                delta_w = alpha * (y_i - y_hat) * x_i

                # Atualização do peso: w_novo = w_antigo + Δw
                w_new = w + delta_w

                # Registro e impressão do log
                update_info = {
                    'epoch': epoch,
                    'example_index': i,
                    'x': x_i.tolist(),
                    'y_true': int(y_i),
                    'net_input': float(net_input),
                    'y_predicted': int(y_hat),
                    'w_old': w.tolist(),
                    'delta_w': delta_w.tolist(),
                    'w_new': w_new.tolist()
                }
                log_updates.append(update_info)

                # Aplica a atualização
                w = w_new

                # Imprime o log de atualização
                print(f"--- Época {epoch}, Exemplo {i} (x={x_i}, y={y_i}) ---")
                print(f"   Net Input: {net_input:.4f}, Predição (y_hat): {y_hat}")
                print(f"   w_antigo: {update_info['w_old']}")
                print(f"   Δw: {update_info['delta_w']}")
                print(f"   w_novo: {update_info['w_new']}\n")


        # Critério de parada: se não houve erros na época, o algoritmo convergiu
        if errors == 0:
            print(f"*** CONVERGÊNCIA na Época {epoch}! Nenhum erro encontrado. ***\n")
            break

        # Se for a última época
        if epoch == epochs and errors > 0:
             print(f"*** TREINAMENTO CONCLUÍDO. Máximo de {epochs} épocas atingido. ***\n")


    return w, log_updates

# --- 1. Dados do Exercício Prático (incluindo o BIAS em X[0] = 1) ---
X_data = np.array([
    [1, 1, 1],  # Ex. 0
    [1, 0, 1],  # Ex. 1
    [1, 0, 0],  # Ex. 2
    [1, 1, 0]   # Ex. 3
], dtype=float)

# Rótulos (y)
y_data = np.array([1, 1, -1, -1], dtype=float)

# --- 2. Parâmetros de Treinamento ---
w_initial = np.array([0, 0, 0], dtype=float)  # Pesos Iniciais (w0, w1, w2)
alpha = 0.5                                  # Taxa de Aprendizagem (α)
max_epochs = 10                              # Número máximo de épocas

# --- 3. Execução do Treinamento ---
final_weights, logs = perceptron_train(
    X=X_data,
    y=y_data,
    w_initial=w_initial,
    alpha=alpha,
    epochs=max_epochs
)

# --- 4. Resultados Finais ---
print("=================================================")
print(f"✅ Treinamento Concluído!")
print(f"Pesos Finais (w = [w0 (bias), w1, w2]): {final_weights.tolist()}")
print("=================================================")